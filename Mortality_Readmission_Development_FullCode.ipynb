{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development as Described in \"Risk Stratification with Explainable Machine Learning for 30-Day Procedure-Related Mortality and 30-Day Unplanned Readmission in Patients with Peripheral Arterial Disease\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meredith Cox; J.C. Panagides; Azadeh Tabari, MD; Sanjeeva Kalva, MD; Jayashree Kalpathy-Cramer, PhD; Dania Daye, MD, PhD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pymrmr\n",
    "import shap\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from lifelines import KaplanMeierFitter, CoxPHFitter, statistics\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, average_precision_score, brier_score_loss, confusion_matrix, classification_report, make_scorer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining useful methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confidence_interval(y_pred,y_true):\n",
    "    '''\n",
    "    Calculates the confidence interval of the AUC score using bootstrapping\n",
    "    Code adapted from: https://stackoverflow.com/questions/19124239/scikit-learn-roc-curve-with-confidence-intervals\n",
    "    '''\n",
    "    n_bootstraps = 1000\n",
    "    rng_seed = 0\n",
    "    bootstrapped_scores = []\n",
    "    \n",
    "    y_true.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    rng = np.random.RandomState(rng_seed)\n",
    "    for i in range(n_bootstraps):\n",
    "        indices = rng.randint(0, len(y_pred), len(y_pred))\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            continue\n",
    "\n",
    "        score = roc_auc_score(y_true[indices], y_pred[indices])\n",
    "        bootstrapped_scores.append(score)\n",
    "\n",
    "    sorted_scores = np.array(bootstrapped_scores)\n",
    "    sorted_scores.sort()\n",
    "\n",
    "    confidence_lower = sorted_scores[int(0.05 * len(sorted_scores))]\n",
    "    confidence_upper = sorted_scores[int(0.95 * len(sorted_scores))]\n",
    "    print(\"Confidence interval for the score: [{:0.3f} - {:0.3}]\".format(\n",
    "        confidence_lower, confidence_upper))\n",
    "    return confidence_lower, confidence_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X_test,y_test,model):\n",
    "    '''\n",
    "    Calculates metrics for evaluation: Accuracy, AUC, AU-PRC, \n",
    "    and shows confusion matric and classification report including\n",
    "    sensitivity and specificity.\n",
    "    '''\n",
    "    probs = model.predict_proba(X_test)\n",
    "    probs = probs[:, 1]\n",
    "    auc = roc_auc_score(y_test, probs)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, probs)\n",
    "    pr = average_precision_score(y_test, probs)\n",
    "\n",
    "    print(\"Accuracy: \" + str(clf_rf.score(X_test, y_test)))\n",
    "    print(\"AUC: \" + str(auc))\n",
    "    print(\"AUPRC: \" + str(pr) + '\\n')\n",
    "\n",
    "    # Getting the decision threshold\n",
    "    # Code from here: https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/\n",
    "\n",
    "    yhat = clf_rf.predict_proba(X_test)\n",
    "    yhat = yhat[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, yhat)\n",
    "    J = tpr - fpr\n",
    "    ix = np.argmax(J)\n",
    "    best_thresh = thresholds[ix]\n",
    "\n",
    "    print('Best Threshold=%f' % (best_thresh))\n",
    "\n",
    "    predicts = (clf_rf.predict_proba(X_test)[:,1] >= best_thresh).astype(int)\n",
    "\n",
    "    print(confusion_matrix(y_test, predicts))\n",
    "    print(classification_report(y_test, predicts, target_names=['0','1']))\n",
    "\n",
    "    get_confidence_interval(probs,y_test)\n",
    "    \n",
    "    return fpr, tpr, auc, probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"NSQIP_LEE_2008-2018_optimpute_train_mortality_readmission.csv\")\n",
    "df_val = pd.read_csv(\"NSQIP_LEE_2008-2018_optimpute_val_mortality_readmission.csv\")\n",
    "df_test = pd.read_csv(\"NSQIP_LEE_2008-2018_optimpute_test_mortality_readmission.csv\")\n",
    "\n",
    "outcome_colnames = ['LEE_ULP', 'LEE_BLEEDING','LEE_MI_STROKE','SUPINFEC','WNDINFD','ORGSPCSSI',\n",
    "                    'DEHIS','OUPNEUMO','REINTUB','PULEMBOL','FAILWEAN','RENAINSF','OPRENAFL',\n",
    "                    'URNINFEC','CNSCVA','CDARREST','CDMI','OTHBLEED','OTHDVT','OTHSYSEP',\n",
    "                    'OTHSESHOCK','THROMBOSIS','AMPUTATION','DEATH']\n",
    "outcomes_train = df_train[outcome_colnames]\n",
    "outcomes_val = df_val[outcome_colnames]\n",
    "outcomes_test = df_test[outcome_colnames]\n",
    "\n",
    "predictors_train = df_train.drop(columns=outcome_colnames)\n",
    "predictors_train = predictors_train.drop(columns=['LEE_AMPUTATION','LEE_WOUND'])\n",
    "\n",
    "predictors_val = df_val.drop(columns=outcome_colnames)\n",
    "predictors_val = predictors_val.drop(columns=['LEE_AMPUTATION','LEE_WOUND'])\n",
    "\n",
    "predictors_test = df_test.drop(columns=outcome_colnames)\n",
    "predictors_test = predictors_test.drop(columns=['LEE_AMPUTATION','LEE_WOUND'])\n",
    "\n",
    "outcomes_train.DEATH = outcomes_train.DEATH.round()\n",
    "outcomes_val.DEATH = outcomes_val.DEATH.round()\n",
    "outcomes_test.DEATH = outcomes_test.DEATH.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_to_test = \"DEATH\" #CHANGE TO THE OUTCOME YOU WANT TO TEST\n",
    "\n",
    "selected_features = ['PRPLATE',\n",
    " 'renalcomorb',\n",
    " 'fnstatus',\n",
    " 'dyspnea',\n",
    " 'Symptoms_Claudication',\n",
    " 'PRBILI',\n",
    " 'LEE_HRF_PHYS',\n",
    " 'pulmcomorb',\n",
    " 'PRALBUM',\n",
    " 'ELECTSURG',\n",
    " 'PRBUN',\n",
    " 'PRWBC',\n",
    " 'PRSGOT',\n",
    " 'PRALKPH',\n",
    " 'Symptoms_Asymptomatic',\n",
    " 'diabetes',\n",
    " 'PRCREAT',\n",
    " 'PRHCT',\n",
    " 'PRINR',\n",
    " 'PRPTT'] #Insert selected features from preprocessing step\n",
    "\n",
    "X_train = predictors_train[selected_features]\n",
    "X_val = predictors_val\n",
    "X_test = predictors_test[selected_features]\n",
    "\n",
    "y_train = outcomes_train[outcome_to_test]\n",
    "y_val = outcomes_val[outcome_to_test]\n",
    "y_test = outcomes_test[outcome_to_test]\n",
    "\n",
    "#Oversample\n",
    "oversample = ADASYN()\n",
    "X, y = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "print(outcome_to_test)\n",
    "\n",
    "# Initialize the random forest with default params, will tune later\n",
    "clf_rf = RandomForestClassifier(n_estimators=100, min_samples_split=2, min_samples_leaf=1,max_depth=4, max_features='auto', bootstrap=True, random_state=0)\n",
    "# Fit the model\n",
    "clf_rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the initial model\n",
    "fpr, tpr, auc, probs = evaluate (X_test,y_test, clf_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set evaluation\n",
    "probs_train = clf_rf.predict_proba(X_train)\n",
    "probs_train = probs_train[:, 1]\n",
    "auc = roc_auc_score(y_train, probs_train)\n",
    "print(\"Accuracy: \" + str(clf_rf.score(X_train, y_train)))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_train, probs_train)\n",
    "print(\"AUC: \" + str(metrics.auc(fpr, tpr)))\n",
    "pr = average_precision_score(y_train, probs_train)\n",
    "print(\"AUPRC: \" + str(pr) + '\\n')\n",
    "\n",
    "predicts_train = (clf_rf.predict_proba(X_train)[:,1] >= .5).astype(int)\n",
    "\n",
    "target_names = ['0','1']\n",
    "print(classification_report(y_train, predicts_train, target_names=target_names))\n",
    "confusion_matrix(y_train, predicts_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 500, num = 5)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'None','log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(4, 7, num = 4)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "def auc_scorer(ground_truth, predictions):\n",
    "    fpr, tpr, _ = roc_curve(ground_truth, predictions[:, 1], pos_label=1)    \n",
    "    return auc(fpr, tpr)\n",
    "\n",
    "my_auc_scorer = make_scorer(auc_scorer, greater_is_better=True, needs_proba=True)\n",
    "\n",
    "param_grid\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 2, n_jobs = -1, verbose = 2,scoring = my_auc_scorer)\n",
    "grid_search.fit(X, y)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The optimal params were the same as the default, so we don't need to evaluate a new model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting ROC curve of the best model (random forest)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, probs)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.figure(dpi=300)\n",
    "plt.title('Receiver Operating Characteristic: 30-day Mortality')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = clf_rf.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "features = X_train.columns\n",
    "print([features[i] for i in indices])\n",
    "print(importances[indices])\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.figure(figsize = ( 7 , 7 ))\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "\n",
    "plt.yticks(range(len(indices)),[features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance',fontsize=14)\n",
    "plt.tick_params(\n",
    "    axis='y',          \n",
    "    which='both',      \n",
    "    left=False,\n",
    "    labelsize=14) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(clf_rf.predict,X_test)\n",
    "rf_shap_values = explainer.shap_values(X_test,nsamples=100)\n",
    "shap.summary_plot(rf_shap_values, X_test.iloc[:5037,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fairness_analysis(all_features_test, y_test, clf_rf, predictors):\n",
    "\n",
    "    X_test_fairness = all_features_test\n",
    "    features_fairness = list(X_test.columns)\n",
    "    features_fairness.extend(['Race_white','SEX','AGE'])\n",
    "    X_test_fairness = X_test_fairness[features_fairness]\n",
    "\n",
    "    y_test_fairness = y_test\n",
    "\n",
    "    X_test_white = X_test_fairness.loc[X_test_fairness['Race_white'] == 1]\n",
    "    X_test_nonwhite = X_test_fairness.loc[X_test_fairness['Race_white'] != 1]\n",
    "\n",
    "    X_test_white = X_test_white.drop(columns=['Race_white','SEX','AGE'])\n",
    "    X_test_nonwhite = X_test_nonwhite.drop(columns=['Race_white','SEX','AGE'])\n",
    "\n",
    "    y_test_white = y_test_fairness.loc[X_test_fairness['Race_white'] == 1]\n",
    "    y_test_nonwhite = y_test_fairness.loc[X_test_fairness['Race_white'] != 1]\n",
    "\n",
    "    X_test_female = X_test_fairness.loc[X_test_fairness['SEX'] == 0]\n",
    "    X_test_male = X_test_fairness.loc[X_test_fairness['SEX'] == 1]\n",
    "\n",
    "    X_test_female = X_test_female.drop(columns=['Race_white','SEX','AGE'])\n",
    "    X_test_male = X_test_male.drop(columns=['Race_white','SEX','AGE'])\n",
    "\n",
    "    y_test_female = y_test_fairness.loc[X_test_fairness['SEX'] == 0]\n",
    "    y_test_male = y_test_fairness.loc[X_test_fairness['SEX'] == 1]\n",
    "\n",
    "    X_test_over_65 = X_test_fairness.loc[X_test_fairness['AGE'] > 65]\n",
    "    X_test_under_65 = X_test_fairness.loc[X_test_fairness['AGE'] <= 65]\n",
    "\n",
    "    X_test_over_65 = X_test_over_65.drop(columns=['Race_white','SEX','AGE'])\n",
    "    X_test_under_65 = X_test_under_65.drop(columns=['Race_white','SEX','AGE'])\n",
    "\n",
    "    y_test_over_65 = y_test_fairness.loc[X_test_fairness['AGE'] > 65]\n",
    "    y_test_under_65 = y_test_fairness.loc[X_test_fairness['AGE'] <= 65]\n",
    "\n",
    "\n",
    "    print(\"White\")\n",
    "    fpr_white, tpr_white, roc_auc_white, preds_white = evaluate(X_test_white,y_test_white,clf_rf)\n",
    "    get_confidence_interval(preds_white,y_test_white)\n",
    "    print(\"Nonwhite\")\n",
    "    fpr_nonwhite, tpr_nonwhite, roc_auc_nonwhite, preds_nonwhite = evaluate(X_test_nonwhite,y_test_nonwhite,clf_rf)\n",
    "    get_confidence_interval(preds_nonwhite,y_test_nonwhite)\n",
    "    print(\"Male\")\n",
    "    fpr_male, tpr_male, roc_auc_male, preds_male = evaluate(X_test_male,y_test_male,clf_rf)\n",
    "    get_confidence_interval(preds_male,y_test_male)\n",
    "    print(\"Female\")\n",
    "    fpr_female, tpr_female, roc_auc_female, preds_female = evaluate(X_test_female,y_test_female,clf_rf)\n",
    "    get_confidence_interval(preds_female,y_test_female)\n",
    "    print(\"Over 65\")\n",
    "    fpr_over_65, tpr_over_65, roc_auc_over_65, preds_over_65 = evaluate(X_test_over_65,y_test_over_65,clf_rf)\n",
    "    get_confidence_interval(preds_over_65,y_test_over_65)\n",
    "    print(\"Under 65\")\n",
    "    fpr_under_65, tpr_under_65, roc_auc_under_65, preds_under_65 = evaluate(X_test_under_65,y_test_under_65,clf_rf)\n",
    "    get_confidence_interval(preds_under_65,y_test_under_65)\n",
    "\n",
    "    fig, axs = plt.subplots(3)\n",
    "    fig.set_dpi(300)\n",
    "\n",
    "    plt.rc('font', family='arial')\n",
    "    axs[0].plot(fpr_white, tpr_white, 'darkblue', label = 'White' % roc_auc_white, linewidth=2)\n",
    "    axs[0].plot(fpr_nonwhite, tpr_nonwhite, 'deepskyblue', label = 'Non-white' % roc_auc_nonwhite, linewidth=2)\n",
    "    axs[0].plot([0, 1], [0, 1],'--',color='red',label='No Skill (Chance)',alpha=.9)\n",
    "    axs[0].set_title(\"White vs. Non-white\", fontsize=11)\n",
    "    axs[0].legend(loc = 'lower right')\n",
    "    axs[0].set_xlim([0, 1])\n",
    "    axs[0].set_ylim([0, 1])\n",
    "    axs[0].set_ylabel('Sensitivity')\n",
    "    axs[0].set_xlabel('Specificity')\n",
    "    axs[0].grid(True,which='major',axis='both',alpha=0.2)\n",
    "\n",
    "    axs[1].plot(fpr_male, tpr_male, 'darkgreen', label = 'Male' % roc_auc_male, linewidth=2)\n",
    "    axs[1].plot(fpr_female, tpr_female, 'lightgreen', label = 'Female' % roc_auc_female, linewidth=2)\n",
    "    axs[1].plot([0, 1], [0, 1],'--',color='red',label='No Skill (Chance)',alpha=.9)\n",
    "    axs[1].set_title(\"Male vs. Female\", fontsize=11)\n",
    "    axs[1].legend(loc = 'lower right')\n",
    "    axs[1].set_xlim([0, 1])\n",
    "    axs[1].set_ylim([0, 1])\n",
    "    axs[1].set_ylabel('Sensitivity')\n",
    "    axs[1].set_xlabel('Specificity')\n",
    "    axs[1].grid(True,which='major',axis='both',alpha=0.2)\n",
    "\n",
    "    axs[2].plot(fpr_over_65, tpr_over_65, 'indigo', label = 'Age >= 65' % roc_auc_over_65, linewidth=2)\n",
    "    axs[2].plot(fpr_under_65, tpr_under_65, 'plum', label = 'Age < 65' % roc_auc_under_65, linewidth=2)\n",
    "    axs[2].plot([0, 1], [0, 1],'--',color='red',label='No Skill (Chance)',alpha=.9)\n",
    "    axs[2].set_title(\"Age 65 and Over vs. Under Age 65\", fontsize=11)\n",
    "    axs[2].legend(loc = 'lower right')\n",
    "    axs[2].set_xlim([0, 1])\n",
    "    axs[2].set_ylim([0, 1])\n",
    "    axs[2].set_ylabel('Sensitivity')\n",
    "    axs[2].set_xlabel('Specificity')\n",
    "    axs[2].grid(True,which='major',axis='both',alpha=0.2)\n",
    "\n",
    "    fig.set_figheight(10)\n",
    "    fig.set_figwidth(5)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return y_test_white, y_test_nonwhite, preds_white, preds_nonwhite, y_test_male, y_test_female, preds_male, preds_female, y_test_over_65, y_test_under_65, preds_over_65, preds_under_65\n",
    "\n",
    "y_test_white, y_test_nonwhite, preds_white, preds_nonwhite, y_test_male, y_test_female, preds_male, preds_female, y_test_over_65, y_test_under_65, preds_over_65, preds_under_65 = fairness_analysis(predictors_test,y_test,clf_rf,selected_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delong's Test for AUC curve difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i y_test_white -i y_test_nonwhite -i preds_white -i preds_nonwhite\n",
    "\n",
    "install.packages(\"pROC\")\n",
    "library(pROC)\n",
    "\n",
    "responsea<- y_test_white\n",
    "responseb<- y_test_nonwhite\n",
    "modela<- preds_white\n",
    "modelb<- preds_nonwhite\n",
    "roca<-roc(responsea,modela)\n",
    "rocb<-roc(responseb,modelb)\n",
    "roc.test(roca,rocb,method=c('delong'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i y_test_male -i y_test_female -i preds_male -i preds_female\n",
    "\n",
    "install.packages(\"pROC\")\n",
    "library(pROC)\n",
    "\n",
    "responsea<- y_test_male\n",
    "responseb<- y_test_female\n",
    "modela<- preds_male\n",
    "modelb<- preds_female\n",
    "roca<-roc(responsea,modela)\n",
    "rocb<-roc(responseb,modelb)\n",
    "roc.test(roca,rocb,method=c('delong'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i y_test_over_65 -i y_test_under_65 -i preds_over_65 -i preds_under_65\n",
    "\n",
    "install.packages(\"pROC\")\n",
    "library(pROC)\n",
    "\n",
    "responsea<- y_test_over_65\n",
    "responseb<- y_test_under_65\n",
    "modela<- preds_over_65\n",
    "modelb<- preds_under_65\n",
    "roca<-roc(responsea,modela)\n",
    "rocb<-roc(responseb,modelb)\n",
    "roc.test(roca,rocb,method=c('delong'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survival = pd.read_csv('survival.csv',index_col='Unnamed: 0')\n",
    "survival.unplanned_readmission = survival.unplanned_readmission.replace(np.nan,0)\n",
    "outcomes_train['unplanned_readmission'] = survival.unplanned_readmission[:7429] #2011-2015\n",
    "outcomes_val['unplanned_readmission'] = survival.unplanned_readmission[7429:9407] #2016\n",
    "outcomes_test['unplanned_readmission'] = survival.unplanned_readmission[9407:14444].reset_index(drop=True) #2017-2018\n",
    "\n",
    "outcome_to_test = 'unplanned_readmission'\n",
    "\n",
    "selected_features = ['PRPLATE',\n",
    " 'MRTAS',\n",
    " 'renalcomorb',\n",
    " 'Symptoms_Claudication',\n",
    " 'diabetes',\n",
    " 'steroid',\n",
    " 'fnstatus',\n",
    " 'wndinf',\n",
    " 'ASA3',\n",
    " 'wtloss',\n",
    " 'PRINR',\n",
    " 'PRALBUM',\n",
    " 'PRCREAT',\n",
    " 'Symptoms_Asymptomatic',\n",
    " 'PRBILI',\n",
    " 'PRSGOT',\n",
    " 'ELECTSURG',\n",
    " 'WND3',\n",
    " 'PRHCT']\n",
    "\n",
    "X_train = predictors_train[selected_features]\n",
    "X_val = predictors_val\n",
    "X_test = predictors_test[selected_features]\n",
    "\n",
    "y_train = outcomes_train[outcome_to_test]\n",
    "y_val = outcomes_val[outcome_to_test]\n",
    "y_test = outcomes_test[outcome_to_test]\n",
    "\n",
    "#Oversample\n",
    "oversample = ADASYN()\n",
    "X, y = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "print(outcome_to_test)\n",
    "\n",
    "# Initialize the random forest with default params, will tune later\n",
    "clf_rf = RandomForestClassifier(n_estimators=100, min_samples_split=2, min_samples_leaf=1,max_depth=4, max_features='auto', bootstrap=True, random_state=0)\n",
    "# Fit the model\n",
    "clf_rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, auc, probs = evaluate (X_test, y_test, clf_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set evaluation\n",
    "probs_train = clf_rf.predict_proba(X_train)\n",
    "probs_train = probs_train[:, 1]\n",
    "auc = roc_auc_score(y_train, probs_train)\n",
    "print(\"Accuracy: \" + str(clf_rf.score(X_train, y_train)))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_train, probs_train)\n",
    "print(\"AUC: \" + str(metrics.auc(fpr, tpr)))\n",
    "pr = average_precision_score(y_train, probs_train)\n",
    "print(\"AUPRC: \" + str(pr) + '\\n')\n",
    "\n",
    "predicts_train = (clf_rf.predict_proba(X_train)[:,1] >= .5).astype(int)\n",
    "\n",
    "target_names = ['0','1']\n",
    "print(classification_report(y_train, predicts_train, target_names=target_names))\n",
    "confusion_matrix(y_train, predicts_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 500, num = 5)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'None','log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(4, 7, num = 4)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "def auc_scorer(ground_truth, predictions):\n",
    "    fpr, tpr, _ = roc_curve(ground_truth, predictions[:, 1], pos_label=1)    \n",
    "    return auc(fpr, tpr)\n",
    "\n",
    "my_auc_scorer = make_scorer(auc_scorer, greater_is_better=True, needs_proba=True)\n",
    "\n",
    "param_grid\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 2, n_jobs = -1, verbose = 2,scoring = my_auc_scorer)\n",
    "grid_search.fit(X, y)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The optimal params were the same as the default, so we don't need to evaluate a new model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting ROC curve of the best model (random forest)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, probs)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.figure(dpi=300)\n",
    "plt.title('Receiver Operating Characteristic: 30-day Mortality')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = clf_rf.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "features = X_train.columns\n",
    "print([features[i] for i in indices])\n",
    "print(importances[indices])\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.figure(figsize = ( 7 , 7 ))\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "\n",
    "plt.yticks(range(len(indices)),[features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance',fontsize=14)\n",
    "plt.tick_params(\n",
    "    axis='y',          \n",
    "    which='both',      \n",
    "    left=False,\n",
    "    labelsize=14) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(clf_rf.predict,X_test)\n",
    "rf_shap_values = explainer.shap_values(X_test,nsamples=100)\n",
    "shap.summary_plot(rf_shap_values, X_test.iloc[:5037,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_white, y_test_nonwhite, preds_white, preds_nonwhite, y_test_male, y_test_female, preds_male, preds_female, y_test_over_65, y_test_under_65, preds_over_65, preds_under_65 = fairness_analysis(predictors_test,y_test,clf_rf,selected_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delong's Test for AUC Curve Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i y_test_white -i y_test_nonwhite -i preds_white -i preds_nonwhite\n",
    "\n",
    "install.packages(\"pROC\")\n",
    "library(pROC)\n",
    "\n",
    "responsea<- y_test_white\n",
    "responseb<- y_test_nonwhite\n",
    "modela<- preds_white\n",
    "modelb<- preds_nonwhite\n",
    "roca<-roc(responsea,modela)\n",
    "rocb<-roc(responseb,modelb)\n",
    "roc.test(roca,rocb,method=c('delong'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i y_test_male -i y_test_female -i preds_male -i preds_female\n",
    "\n",
    "install.packages(\"pROC\")\n",
    "library(pROC)\n",
    "\n",
    "responsea<- y_test_male\n",
    "responseb<- y_test_female\n",
    "modela<- preds_male\n",
    "modelb<- preds_female\n",
    "roca<-roc(responsea,modela)\n",
    "rocb<-roc(responseb,modelb)\n",
    "roc.test(roca,rocb,method=c('delong'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i y_test_over_65 -i y_test_under_65 -i preds_over_65 -i preds_under_65\n",
    "\n",
    "install.packages(\"pROC\")\n",
    "library(pROC)\n",
    "\n",
    "responsea<- y_test_over_65\n",
    "responseb<- y_test_under_65\n",
    "modela<- preds_over_65\n",
    "modelb<- preds_under_65\n",
    "roca<-roc(responsea,modela)\n",
    "rocb<-roc(responseb,modelb)\n",
    "roc.test(roca,rocb,method=c('delong'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survival Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def survival_binary(feature_name,outcome_name,survival_name,labels, title):\n",
    "    '''\n",
    "    Determining differences in survival for binary features (ones that do not require definition of a threshold)\n",
    "    '''\n",
    "\n",
    "    feature_all=pd.concat([predictors_train, predictors_val, predictors_test], axis=0)[feature_name].round().reset_index(drop=True)\n",
    "    \n",
    "    survival_1 = survival.loc[feature_all == 1]\n",
    "    died_1 = outcomes_all.loc[feature_all == 1][outcome_name]\n",
    "    survival_0 = survival.loc[feature_all != 1]\n",
    "    died_0 = outcomes_all.loc[feature_all != 1][outcome_name]\n",
    "\n",
    "    kmf_1 = KaplanMeierFitter()\n",
    "    X_1 = survival_1[survival_name]\n",
    "    Y_1 = died_1\n",
    "\n",
    "    kmf_0 = KaplanMeierFitter()\n",
    "    X_0 = survival_0[survival_name]\n",
    "    Y_0 = died_0\n",
    "\n",
    "    kmf_1.fit_right_censoring(X_1, event_observed = Y_1)\n",
    "    kmf_0.fit_right_censoring(X_0, event_observed = Y_0)\n",
    "    plt.figure(dpi=300)\n",
    "    plt.ylabel(\"Survival\")\n",
    "    kmf_1.plot(label=labels[0])\n",
    "    kmf_0.plot(label=labels[1])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Days after procedure\")\n",
    "    \n",
    "    results = statistics.logrank_test(X_1, X_0, event_observed_A=Y_1, event_observed_B=Y_0)\n",
    "    print(\"Log-rank test p-value: \", results.p_value) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def survival_continuous(feature_name,outcome_name,survival_name,labels, title, threshold):\n",
    "    '''\n",
    "    Determining differences in survival for continuous features (ones that require definition of a threshold)\n",
    "    '''\n",
    "\n",
    "    feature_all=pd.concat([predictors_train, predictors_val, predictors_test], axis=0)[feature_name].reset_index(drop=True)\n",
    "\n",
    "    survival_0 = survival.loc[feature_all < threshold] \n",
    "    died_0 = outcomes_all.loc[feature_all < threshold][outcome_name]\n",
    "    survival_1 = survival.loc[feature_all >= threshold]\n",
    "    died_1 = outcomes_all.loc[feature_all >= threshold][outcome_name]\n",
    "\n",
    "    kmf_0 = KaplanMeierFitter()\n",
    "    X_0 = survival_0[survival_name]\n",
    "    Y_0 = died_0\n",
    "\n",
    "    kmf_1 = KaplanMeierFitter()\n",
    "    X_1 = survival_1[survival_name]\n",
    "    Y_1 = died_1\n",
    "\n",
    "    plt.figure(dpi=300)\n",
    "    kmf_0.fit_right_censoring(X_0, event_observed = Y_0)\n",
    "    kmf_1.fit_right_censoring(X_1, event_observed = Y_1)\n",
    "    plt.ylabel(\"Survival\")\n",
    "    plt.title(title)\n",
    "    kmf_0.plot(label=str(feature_name+' < '+ str(threshold)))\n",
    "    kmf_1.plot(label=str(feature_name+' >= '+ str(threshold)))\n",
    "    plt.xlabel(\"Days after procedure\")\n",
    "\n",
    "    results = statistics.logrank_test(X_1, X_0, event_observed_A=Y_1, event_observed_B=Y_0)\n",
    "    print(\"Log-rank test p-value: \", results.p_value) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survival.DOPERTOD = survival.DOPERTOD.replace(-99.0,30) #Convert -99 to 30 since we are looking at 30-day outcomes\n",
    "survival['dead'] = survival.DOPERTOD.notnull()\n",
    "outcomes_all = pd.concat([outcomes_train, outcomes_val, outcomes_test], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmf = KaplanMeierFitter()\n",
    "X= survival['DOPERTOD']\n",
    "Y = outcomes_all.DEATH\n",
    "\n",
    "kmf.fit_right_censoring(X, event_observed = Y)\n",
    "kmf.plot()\n",
    "plt.title(\"Kaplan Meier estimates: 30-day Mortality\")\n",
    "plt.xlabel(\"Days after procedure\")\n",
    "plt.ylabel(\"Survival\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting survival curves between two groups\n",
    "\n",
    "survival_binary('ELECTSURG','DEATH','DOPERTOD',('Elective Surgery','Nonelective Surgery'),'Elective Surgery')\n",
    "survival_binary('LEE_HRF_PHYS','DEATH','DOPERTOD',('High Risk Factors','No High Risk Factors'),'Physiologic High Risk Factors')\n",
    "survival_binary('fnstatus','DEATH','DOPERTOD',('Independent','Totally or Partially Dependent'),'Functional Status')\n",
    "survival_continuous('PRHCT','DEATH','DOPERTOD',('HCT < 30','HCT >= 30'),'Hematocrit',30)\n",
    "survival_continuous('PRCREAT','DEATH','DOPERTOD',('Creatinine < 30','Creatinine >= 30'),'Creatinine',1.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survival.READMPODAYS1 = survival.READMPODAYS1.replace(-99.0,np.nan)\n",
    "survival.READMPODAYS1 = survival.READMPODAYS1.replace(np.nan,30)\n",
    "survival.unplanned_readmission = survival.unplanned_readmission.replace(np.nan,0)\n",
    "\n",
    "outcomes_all['unplanned_readmission'] = survival.unplanned_readmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmf3 = KaplanMeierFitter()\n",
    "X= survival['READMPODAYS1']\n",
    "Y= survival.unplanned_readmission\n",
    "\n",
    "kmf3.fit_right_censoring(X, event_observed = Y)\n",
    "kmf3.plot()\n",
    "plt.title(\"Kaplan Meier estimates: 30-day Readmission\")\n",
    "plt.xlabel(\"Days after procedure\")\n",
    "plt.ylabel(\"Survival\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting survival curves between two groups\n",
    "\n",
    "survival_binary('wndinf','unplanned_readmission','READMPODAYS1',('Open Wound/Wound Infection','No Open Wound/Wound Infection'),'Open Wound/Wound Infection')\n",
    "survival_binary('ELECTSURG','unplanned_readmission','READMPODAYS1',('Elective Surgery','Nonelective Surgery'),'Elective Surgery')\n",
    "survival_binary('diabetes','unplanned_readmission','READMPODAYS1',('Diabetic','Non-diabetic'),'Diabetes')\n",
    "survival_binary('Symptoms_Claudication','unplanned_readmission','READMPODAYS1',('Claudication','No Claudication'),'Claudication')\n",
    "survival_death_binary('MRTAS','unplanned_readmission','READMPODAYS1',('MRTAS','No MRTAS'),'Major Reintervention of Treated Arterial Segment (MRTAS)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
